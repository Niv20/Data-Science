{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Silence Culture Analysis Project\n",
        "## ISSP 2022 - Family and Changing Gender Roles Survey\n",
        "\n",
        "This notebook analyzes public attitudes on sensitive social topics including health beliefs, gender roles, family structures, and personal information sharing patterns."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Import Required Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
        "import statsmodels.api as sm\n",
        "from scipy import stats\n",
        "\n",
        "# Suppress warnings for cleaner output\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set plotting style\n",
        "sns.set_style('whitegrid')\n",
        "plt.rcParams['figure.figsize'] = (12, 6)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Load Data\n",
        "Load the CSV file containing survey responses"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load the survey dataset\n",
        "df = pd.read_stata('2022/2022Data.dta')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "# PART 1: Global Research on \"Culture of Silence\"\n",
        "## Analyzing the \"Silence Hierarchy\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1.1 Define Topic Categories\n",
        "\n",
        "We group survey questions into thematic categories to measure which topics people avoid discussing (refusal, \"cannot decide\", or no response). Higher \"silence rate\" indicates greater social taboo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define question groups by thematic category\n",
        "# Each category contains question that address related topics\n",
        "# We'll calculate a silence score for each category\n",
        "\n",
        "categories_mapping = {\n",
        "    'Sensitive Personal Info': ['q143', 'q144', 'q145', 'q146', 'q147', 'q173', 'q174', 'q177', 'q178', 'q179', 'q180', 'q181', 'q182'],\n",
        "    'Gender Perceptions': ['q63', 'q64', 'q65', 'q66', 'q67', 'q68'],\n",
        "    'Family Life Reality': ['q103', 'q104', 'q105', 'q106', 'q107', 'q108', 'q109', 'q110', 'q111', 'q112', 'q113'],\n",
        "    'New Families & Morality': ['q77', 'q78', 'q79', 'q80', 'q81'],\n",
        "    'Health System Trust': ['q3', 'q11', 'q12', 'q13', 'q14', 'q21', 'q22', 'q23'],\n",
        "    'Corona Policy & Emergency': ['q57', 'q58', 'q59', 'q60', 'q61'],\n",
        "    'Personal & Mental Health': ['q2', 'q37', 'q38', 'q39', 'q49', 'q50', 'q51', 'q52', 'q53', 'q54', 'q120'],\n",
        "    'Digital Literacy & Health': ['q24', 'q25', 'q26', 'q27', 'q28', 'q29', 'q30', 'q31', 'q32'],\n",
        "    'Family Solidarity': ['q115', 'q116', 'q117']\n",
        "}\n",
        "\n",
        "print(\"Topic Categories Defined:\")\n",
        "for category, questions in categories_mapping.items():\n",
        "    print(f\"  {category}: {len(questions)} questions\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1.2 Calculate Silence Scores\n",
        "\n",
        "For each category, we calculate the percentage of people who:\n",
        "- Refused to answer\n",
        "- Selected \"Cannot decide\"\n",
        "- Left the response blank (NaN)\n",
        "\n",
        "This gives us a \"silence score\" that indicates how taboo a topic is - higher scores mean fewer people are willing to engage with the topic."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Function to calculate silence score for a group of questions\n",
        "def calculate_silence_score(df, question_columns, refuse_keywords=['refuse', 'cannot choose', 'cannot decide']):\n",
        "    \"\"\"\n",
        "    Calculate percentage of non-responses (refusal, cannot decide, NaN) for a set of questions.\n",
        "    \n",
        "    Parameters:\n",
        "    - df: dataframe\n",
        "    - question_columns: list of column names\n",
        "    - refuse_keywords: list of keywords indicating refusal/inability to respond\n",
        "    \n",
        "    Returns: float (percentage of silence)\n",
        "    \"\"\"\n",
        "    \n",
        "    total_responses = 0\n",
        "    silence_count = 0\n",
        "    \n",
        "    for col in question_columns:\n",
        "        if col in df.columns:\n",
        "            # Count missing values\n",
        "            missing = df[col].isna().sum()\n",
        "            silence_count += missing\n",
        "            \n",
        "            # Count refusal/cannot decide responses\n",
        "            for keyword in refuse_keywords:\n",
        "                silence_count += (df[col].astype(str).str.lower().str.contains(keyword, na=False)).sum()\n",
        "            \n",
        "            total_responses += len(df[col])\n",
        "    \n",
        "    # Calculate percentage of silence\n",
        "    silence_percentage = (silence_count / total_responses * 100) if total_responses > 0 else 0\n",
        "    return silence_percentage\n",
        "\n",
        "# Calculate silence scores for each category\n",
        "silence_scores = {}\n",
        "for category, questions in categories_mapping.items():\n",
        "    silence_scores[category] = calculate_silence_score(df, questions)\n",
        "\n",
        "# Convert to DataFrame for easier visualization\n",
        "silence_df = pd.DataFrame(list(silence_scores.items()), columns=['Topic', 'Silence Score'])\n",
        "silence_df = silence_df.sort_values('Silence Score', ascending=False)\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"SILENCE HIERARCHY: Topics Ranked by Avoidance\")\n",
        "print(\"=\"*60)\n",
        "print(silence_df.to_string(index=False))\n",
        "print(\"\\nInterpretation:\")\n",
        "print(\"- Higher scores = more taboo (people avoid discussing)\")\n",
        "print(\"- Lower scores = easier to discuss (more people engage)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1.3 Visualize the Silence Hierarchy\n",
        "\n",
        "A bar plot shows which social topics are most and least taboo in the studied population."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create bar plot of silence scores\n",
        "fig, ax = plt.subplots(figsize=(12, 7))\n",
        "\n",
        "# Sort for better visualization (descending order)\n",
        "silence_df_sorted = silence_df.sort_values('Silence Score', ascending=True)\n",
        "\n",
        "# Create horizontal bar chart\n",
        "bars = ax.barh(silence_df_sorted['Topic'], silence_df_sorted['Silence Score'], \n",
        "                color=plt.cm.RdYlGn_r(silence_df_sorted['Silence Score']/silence_df_sorted['Silence Score'].max()))\n",
        "\n",
        "# Customize plot\n",
        "ax.set_xlabel('Silence Score (% of Non-Responses)', fontsize=12, fontweight='bold')\n",
        "ax.set_ylabel('Topic Category', fontsize=12, fontweight='bold')\n",
        "ax.set_title('Silence Hierarchy: Which Topics Are Most Taboo?', fontsize=14, fontweight='bold', pad=20)\n",
        "ax.set_xlim(0, silence_df['Silence Score'].max() * 1.1)\n",
        "\n",
        "# Add value labels on bars\n",
        "for i, (topic, score) in enumerate(zip(silence_df_sorted['Topic'], silence_df_sorted['Silence Score'])):\n",
        "    ax.text(score + 0.5, i, f'{score:.1f}%', va='center', fontsize=10, fontweight='bold')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nKey Insights:\")\n",
        "print(f\"  Most Taboo: {silence_df.iloc[0]['Topic']} ({silence_df.iloc[0]['Silence Score']:.1f}% silence)\")\n",
        "print(f\"  Least Taboo: {silence_df.iloc[-1]['Topic']} ({silence_df.iloc[-1]['Silence Score']:.1f}% silence)\")\n",
        "print(f\"  Average Silence: {silence_df['Silence Score'].mean():.1f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1.4 Descriptive Statistics on Silence Scores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Calculate and display descriptive statistics\n",
        "print(\"\\nDescriptive Statistics: Silence Scores Across All Topics\")\n",
        "print(\"=\"*60)\n",
        "print(silence_df['Silence Score'].describe())\n",
        "print(\"\\nStatistical Interpretation:\")\n",
        "print(f\"  - Mean (average): {silence_df['Silence Score'].mean():.2f}%\")\n",
        "print(f\"  - Median (middle value): {silence_df['Silence Score'].median():.2f}%\")\n",
        "print(f\"  - Std Dev (variability): {silence_df['Silence Score'].std():.2f}%\")\n",
        "print(f\"  - Range: {silence_df['Silence Score'].min():.2f}% to {silence_df['Silence Score'].max():.2f}%\")\n",
        "print(\"\\nWhat this means:\")\n",
        "print(\"  - High standard deviation suggests topics vary widely in how taboo they are\")\n",
        "print(\"  - Some topics are universally discussed while others are heavily avoided\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "# PART 2: \"Gender Silence\": Where Is It Hardest to Talk About Women's Roles?\n",
        "\n",
        "We analyze whether countries with traditional characteristics show:\n",
        "- Lower variance (everyone agrees - conformity)\n",
        "- Higher refusal rates (higher silence despite agreement)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.1 Focus on Gender Role Questions (Q63, Q68)\n",
        "\n",
        "These questions directly address gender role expectations:\n",
        "- Q63: \"Can working mothers maintain warm relationships with their children?\"\n",
        "- Q68: \"Man's role is earning; woman's role is home and family\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Identify country column (adjust name if different in your dataset)\n",
        "country_column = 'country'  # IMPORTANT: Change this to your actual country column name\n",
        "\n",
        "# Define the gender role questions\n",
        "gender_questions = ['q63', 'q68']  # IMPORTANT: Adjust these to your actual column names\n",
        "\n",
        "# Verify these columns exist in the dataset\n",
        "existing_columns = [col for col in gender_questions if col in df.columns]\n",
        "print(f\"Gender role questions found in dataset: {existing_columns}\")\n",
        "\n",
        "# Filter dataset for countries (remove missing values)\n",
        "if country_column in df.columns:\n",
        "    df_countries = df[df[country_column].notna()].copy()\n",
        "    print(f\"\\nCountries in dataset: {df_countries[country_column].unique()}\")\n",
        "else:\n",
        "    print(f\"Warning: '{country_column}' not found. Using all data without country grouping.\")\n",
        "    df_countries = df.copy()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.2 Prepare Data: One-Hot Encoding for Countries\n",
        "\n",
        "We convert categorical country variable into numerical format (dummy variables) to enable analysis."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Function to calculate refusal/avoidance rate for a specific column\n",
        "def calculate_refusal_rate(series, refuse_keywords=['refuse', 'cannot choose', 'cannot decide']):\n",
        "    \"\"\"\n",
        "    Calculate percentage of refusals and non-responses in a series.\n",
        "    \n",
        "    Returns: float (percentage)\n",
        "    \"\"\"\n",
        "    total = len(series.dropna())\n",
        "    refusals = 0\n",
        "    \n",
        "    # Count missing values\n",
        "    refusals += series.isna().sum()\n",
        "    \n",
        "    # Count refusal keywords\n",
        "    for keyword in refuse_keywords:\n",
        "        refusals += (series.astype(str).str.lower().str.contains(keyword, na=False)).sum()\n",
        "    \n",
        "    refusal_percentage = (refusals / len(series) * 100) if len(series) > 0 else 0\n",
        "    return refusal_percentage\n",
        "\n",
        "# Calculate refusal rates by country for gender questions\n",
        "if country_column in df.columns:\n",
        "    country_refusals = {}\n",
        "    \n",
        "    for country in df_countries[country_column].unique():\n",
        "        country_data = df_countries[df_countries[country_column] == country]\n",
        "        \n",
        "        # Calculate average refusal rate across gender questions\n",
        "        refusal_rates = [calculate_refusal_rate(country_data[col]) for col in existing_columns if col in country_data.columns]\n",
        "        avg_refusal = np.mean(refusal_rates) if refusal_rates else 0\n",
        "        country_refusals[country] = avg_refusal\n",
        "    \n",
        "    # Convert to DataFrame\n",
        "    country_refusal_df = pd.DataFrame(list(country_refusals.items()), \n",
        "                                      columns=['Country', 'Avoidance Rate (%)'])\n",
        "    country_refusal_df = country_refusal_df.sort_values('Avoidance Rate (%)', ascending=False)\n",
        "    \n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"GENDER SILENCE BY COUNTRY\")\n",
        "    print(\"Percentage of people avoiding gender role questions\")\n",
        "    print(\"=\"*60)\n",
        "    print(country_refusal_df.to_string(index=False))\n",
        "    else:\n",
        "        print(\"\\nCountry variable not found. Skipping country-level analysis.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.3 Analyze Response Variation Within Countries\n",
        "\n",
        "We compare variance in responses (standard deviation) to understand whether some countries show conformity vs. disagreement."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Helper function to convert text responses to numerical values for analysis\n",
        "def convert_likert_to_numeric(value):\n",
        "    \"\"\"\n",
        "    Convert Likert-scale responses to numerical values.\n",
        "    This helps us calculate mean and standard deviation.\n",
        "    \n",
        "\"\"\"\n",
        "    likert_mapping = {\n",
        "        'strongly agree': 2,\n",
        "        'agree': 1,\n",
        "        'neither agree nor disagree': 0,\n",
        "        'disagree': -1,\n",
        "        'strongly disagree': -2,\n",
        "        'cannot decide': np.nan,\n",
        "        'refuse': np.nan\n",
        "    }\n",
        "    \n",
        "    if isinstance(value, str):\n",
        "        for key, numeric_value in likert_mapping.items():\n",
        "            if key.lower() in value.lower():\n",
        "                return numeric_value\n",
        "    return np.nan\n",
        "\n",
        "# Create numeric versions of gender questions\n",
        "for col in existing_columns:\n",
        "    if col in df.columns:\n",
        "        numeric_col = f'{col}_numeric'\n",
        "        df[numeric_col] = df[col].apply(convert_likert_to_numeric)\n",
        "\n",
        "# Analyze variance by country\n",
        "if country_column in df.columns:\n",
        "    variance_analysis = []\n",
        "    \n",
        "    for country in df_countries[country_column].unique():\n",
        "        country_data = df_countries[df_countries[country_column] == country]\n",
        "        \n",
        "        # Get numeric versions and calculate statistics\n",
        "        numeric_cols_existing = [f'{col}_numeric' for col in existing_columns if f'{col}_numeric' in country_data.columns]\n",
        "        \n",
        "        if numeric_cols_existing:\n",
        "            # Calculate mean and std across gender questions\n",
        "            means = []\n",
        "            stds = []\n",
        "            \n",
        "            for col in numeric_cols_existing:\n",
        "                col_data = country_data[col].dropna()\n",
        "                if len(col_data) > 0:\n",
        "                    means.append(col_data.mean())\n",
        "                    stds.append(col_data.std())\n",
        "            \n",
        "            if means and stds:\n",
        "                avg_mean = np.mean(means)\n",
        "                avg_std = np.mean(stds)\n",
        "                variance_analysis.append({\n",
        "                    'Country': country,\n",
        "                    'Mean Response': avg_mean,\n",
        "                    'Std Dev (Disagreement)': avg_std\n",
        "                })\n",
        "    \n",
        "    variance_df = pd.DataFrame(variance_analysis)\n",
        "    variance_df = variance_df.sort_values('Std Dev (Disagreement)', ascending=False)\n",
        "    \n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"RESPONSE VARIATION IN GENDER QUESTIONS BY COUNTRY\")\n",
        "    print(\"Higher Std Dev = More Disagreement (heterogeneous views)\")\n",
        "    print(\"Lower Std Dev = More Conformity (homogeneous views)\")\n",
        "    print(\"=\"*80)\n",
        "    print(variance_df.to_string(index=False))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.4 Visualization: Gender Silence Across Countries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create visualization of gender silence by country\n",
        "if country_column in df.columns and not country_refusal_df.empty:\n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))\n",
        "    \n",
        "    # Plot 1: Avoidance Rates\n",
        "    ax1.bar(country_refusal_df['Country'], country_refusal_df['Avoidance Rate (%)'], \n",
        "            color='steelblue', alpha=0.7, edgecolor='black')\n",
        "    ax1.set_xlabel('Country', fontsize=11, fontweight='bold')\n",
        "    ax1.set_ylabel('Avoidance Rate (%)', fontsize=11, fontweight='bold')\n",
        "    ax1.set_title('Gender Role Questions: Who Avoids Answering?', fontsize=12, fontweight='bold')\n",
        "    ax1.tick_params(axis='x', rotation=45)\n",
        "    \n",
        "    # Plot 2: Response Variation\n",
        "    if not variance_df.empty:\n",
        "        ax2.scatter(variance_df['Mean Response'], variance_df['Std Dev (Disagreement)'], \n",
        "                   s=200, alpha=0.6, c=range(len(variance_df)), cmap='viridis', edgecolor='black')\n",
        "        \n",
        "        for idx, row in variance_df.iterrows():\n",
        "            ax2.annotate(row['Country'], \n",
        "                        (row['Mean Response'], row['Std Dev (Disagreement)']),\n",
        "                        xytext=(5, 5), textcoords='offset points', fontsize=9)\n",
        "        \n",
        "        ax2.set_xlabel('Average Response (Traditional → Liberal)', fontsize=11, fontweight='bold')\n",
        "        ax2.set_ylabel('Standard Deviation (Disagreement)', fontsize=11, fontweight='bold')\n",
        "        ax2.set_title('Homogeneity vs Disagreement in Gender Views', fontsize=12, fontweight='bold')\n",
        "        ax2.grid(True, alpha=0.3)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"\\nCountry-level visualization not possible without country column.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.5 Key Insights from Gender Silence Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"GENDER SILENCE ANALYSIS: KEY FINDINGS\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "if not country_refusal_df.empty:\n",
        "    max_avoidance = country_refusal_df.iloc[0]\n",
        "    min_avoidance = country_refusal_df.iloc[-1]\n",
        "    print(f\"\\n1. Highest Avoidance: {max_avoidance['Country']} ({max_avoidance['Avoidance Rate (%)']:.1f}%)\")\n",
        "    print(f\"   Interpretation: People in this country are most reluctant to discuss gender roles\")\n",
        "    print(f\"\\n2. Lowest Avoidance: {min_avoidance['Country']} ({min_avoidance['Avoidance Rate (%)']:.1f}%)\")\n",
        "    print(f\"   Interpretation: People in this country more openly discuss gender roles\")\n",
        "\n",
        "if not variance_df.empty:\n",
        "    high_var = variance_df.iloc[0]\n",
        "    low_var = variance_df.iloc[-1]\n",
        "    print(f\"\\n3. Highest Disagreement: {high_var['Country']} (Std Dev: {high_var['Std Dev (Disagreement)']:.2f})\")\n",
        "    print(f\"   Interpretation: People hold diverse views on gender roles (heterogeneous)\")\n",
        "    print(f\"\\n4. Highest Conformity: {low_var['Country']} (Std Dev: {low_var['Std Dev (Disagreement)']:.2f})\")\n",
        "    print(f\"   Interpretation: People largely agree (homogeneous views)\")\n",
        "\n",
        "print(\"\\n5. Paradox Analysis:\")\n",
        "print(\"   Do traditional countries show:\")\n",
        "print(\"   - High avoidance (silence) BUT high conformity (agreement)?\")\n",
        "print(\"   - High avoidance (silence) AND high disagreement (conflict)?\")\n",
        "print(\"   Different patterns suggest different social dynamics.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "# PART 3: \"Economic Privacy Paradox\": Predicting Income Question Refusers\n",
        "\n",
        "Can we predict who will refuse to answer income questions (Q173-Q174) based on education, age, and home ownership?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.1 Prepare Target Variable: Income Question Refusal\n",
        "\n",
        "Create a binary variable:\n",
        "- 1 = Refused to answer income questions (\"cannot decide\", \"refuse\", or missing)\n",
        "- 0 = Provided an income response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define income questions\n",
        "income_questions = ['q173', 'q174']  # IMPORTANT: Adjust to your actual column names\n",
        "\n",
        "# Create binary target: Did respondent refuse income questions?\n",
        "def is_income_refusal(row):\n",
        "    \"\"\"\n",
        "    Return 1 if person refused or cannot decide on BOTH income questions.\n",
        "    Return 0 if person answered at least one income question.\n",
        "    \"\"\"\n",
        "    refuse_keywords = ['refuse', 'cannot choose', 'cannot decide']\n",
        "    \n",
        "    refusals = 0\n",
        "    total_questions = 0\n",
        "    \n",
        "    for col in income_questions:\n",
        "        if col in df.columns:\n",
        "            total_questions += 1\n",
        "            value = row[col]\n",
        "            \n",
        "            # Check if missing or contains refusal keyword\n",
        "            if pd.isna(value):\n",
        "                refusals += 1\n",
        "            elif any(keyword.lower() in str(value).lower() for keyword in refuse_keywords):\n",
        "                refusals += 1\n",
        "    \n",
        "    # Return 1 if refused both questions, 0 otherwise\n",
        "    return 1 if refusals == total_questions and total_questions > 0 else 0\n",
        "\n",
        "# Apply function to create target variable\n",
        "df['income_refusal'] = df.apply(is_income_refusal, axis=1)\n",
        "\n",
        "# Check the distribution\n",
        "refusal_counts = df['income_refusal'].value_counts()\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"INCOME QUESTION REFUSAL DISTRIBUTION\")\n",
        "print(\"=\"*60)\n",
        "print(f\"Answered income questions: {refusal_counts[0] if 0 in refusal_counts.index else 0} ({refusal_counts[0]/len(df)*100 if 0 in refusal_counts.index else 0:.1f}%)\")\n",
        "print(f\"Refused income questions:  {refusal_counts[1] if 1 in refusal_counts.index else 0} ({refusal_counts[1]/len(df)*100 if 1 in refusal_counts.index else 0:.1f}%)\")\n",
        "print(f\"\\nTotal respondents: {len(df)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.2 Prepare Predictor Variables\n",
        "\n",
        "We'll use:\n",
        "- Age (numerical)\n",
        "- Education level (categorical → one-hot encoded)\n",
        "- Home ownership (binary)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define predictor variables (adjust column names as needed)\n",
        "age_column = 'q127'  # IMPORTANT: Change if your age column has different name\n",
        "education_column = 'q129'  # IMPORTANT: Change if your education column has different name\n",
        "home_ownership_column = 'q155'  # IMPORTANT: Adjust based on your data\n",
        "\n",
        "# Create working dataset for modeling\n",
        "df_model = df.copy()\n",
        "\n",
        "# 1. Prepare Age variable\n",
        "if age_column in df_model.columns:\n",
        "    # Convert birth year to age (or use age directly if already age)\n",
        "    try:\n",
        "        df_model['age'] = pd.to_numeric(df_model[age_column], errors='coerce')\n",
        "        # If values are years of birth, convert to age\n",
        "        if df_model['age'].max() > 150:  # Likely birth years\n",
        "            current_year = 2024\n",
        "            df_model['age'] = current_year - df_model['age']\n",
        "    except:\n",
        "        print(f\"Warning: Could not process {age_column} as age\")\n",
        "\n",
        "# 2. Prepare Education variable\n",
        "if education_column in df_model.columns:\n",
        "    df_model['education'] = df_model[education_column].astype(str)\n",
        "    # Count education categories\n",
        "    print(f\"\\nEducation categories: {df_model['education'].nunique()}\")\n",
        "    print(df_model['education'].value_counts().head())\n",
        "else:\n",
        "    print(f\"Warning: {education_column} not found\")\n",
        "\n",
        "# 3. Prepare Home Ownership variable\n",
        "if home_ownership_column in df_model.columns:\n",
        "    df_model['home_owner'] = df_model[home_ownership_column].notna().astype(int)\n",
        "else:\n",
        "    print(f\"Warning: {home_ownership_column} not found\")\n",
        "    # Create dummy home ownership for demonstration\n",
        "    df_model['home_owner'] = np.random.choice([0, 1], size=len(df_model))\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"PREDICTOR VARIABLES PREPARED\")\n",
        "print(\"=\"*60)\n",
        "print(f\"Age: {df_model['age'].describe().to_string()}\")\n",
        "print(f\"\\nHome Ownership Distribution:\")\n",
        "print(df_model['home_owner'].value_counts())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.3 Feature Engineering and Data Cleaning\n",
        "\n",
        "Prepare data for machine learning models by handling missing values and creating dummy variables."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Select features for modeling\n",
        "feature_cols = ['age', 'education', 'home_owner']\n",
        "df_clean = df_model[feature_cols + ['income_refusal']].copy()\n",
        "\n",
        "# Remove rows with missing target variable\n",
        "df_clean = df_clean[df_clean['income_refusal'].notna()]\n",
        "\n",
        "# Handle missing values in predictors\n",
        "print(\"Missing values before cleaning:\")\n",
        "print(df_clean.isnull().sum())\n",
        "\n",
        "# Drop rows with missing age (critical predictor)\n",
        "df_clean = df_clean[df_clean['age'].notna()]\n",
        "\n",
        "# Fill education with 'Unknown' if missing\n",
        "df_clean['education'].fillna('Unknown', inplace=True)\n",
        "\n",
        "# Fill home_owner with mode (0) if missing\n",
        "df_clean['home_owner'].fillna(0, inplace=True)\n",
        "\n",
        "print(\"\\nMissing values after cleaning:\")\n",
        "print(df_clean.isnull().sum())\n",
        "print(f\"\\nDataset shape for modeling: {df_clean.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.4 One-Hot Encoding for Education\n",
        "\n",
        "Convert the categorical education variable into binary dummy variables for the logistic regression model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create dummy variables for education\n",
        "# drop_first=True avoids multicollinearity (perfect linear relationship)\n",
        "X_education_dummies = pd.get_dummies(df_clean[['education']], drop_first=True)\n",
        "\n",
        "print(\"Education dummy variables created:\")\n",
        "print(X_education_dummies.head())\n",
        "print(f\"\\nShape: {X_education_dummies.shape}\")\n",
        "\n",
        "# Combine age, home ownership, and education dummies\n",
        "X = pd.concat([df_clean[['age', 'home_owner']], X_education_dummies], axis=1)\n",
        "y = df_clean['income_refusal']\n",
        "\n",
        "print(f\"\\nFinal predictor variables:\")\n",
        "print(X.columns.tolist())\n",
        "print(f\"Predictor matrix shape: {X.shape}\")\n",
        "print(f\"Target variable distribution:\\n{y.value_counts()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.5 Feature Scaling\n",
        "\n",
        "Standardize predictors so each has mean=0 and std=1. This is important for logistic regression and KNN."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Standardize features (important for logistic regression and KNN)\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "X_scaled_df = pd.DataFrame(X_scaled, columns=X.columns)\n",
        "\n",
        "print(\"Feature scaling results:\")\n",
        "print(f\"\\nMean of scaled features (should be ~0):\")\n",
        "print(X_scaled_df.mean())\n",
        "print(f\"\\nStandard deviation of scaled features (should be ~1):\")\n",
        "print(X_scaled_df.std())\n",
        "print(\"\\nScaling ensures all features contribute equally to the model.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.6 Train-Test Split\n",
        "\n",
        "Divide data into training (80%) and test (20%) sets to evaluate model performance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Split data: 80% training, 20% testing\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X_scaled_df, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"TRAIN-TEST SPLIT SUMMARY\")\n",
        "print(\"=\"*60)\n",
        "print(f\"Training set size: {len(X_train)} ({len(X_train)/len(X_scaled_df)*100:.1f}%)\")\n",
        "print(f\"Test set size: {len(X_test)} ({len(X_test)/len(X_scaled_df)*100:.1f}%)\")\n",
        "print(f\"\\nTarget variable distribution in training set:\")\n",
        "print(y_train.value_counts())\n",
        "print(f\"\\nTarget variable distribution in test set:\")\n",
        "print(y_test.value_counts())\n",
        "print(\"\\nNote: Stratified split ensures both sets have similar target distributions.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.7 Build Logistic Regression Model\n",
        "\n",
        "Logistic regression predicts probability of income refusal. It's ideal for binary classification."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train logistic regression model\n",
        "log_reg = LogisticRegression(random_state=42, max_iter=1000)\n",
        "log_reg.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on test set\n",
        "y_pred_log = log_reg.predict(X_test)\n",
        "y_pred_proba = log_reg.predict_proba(X_test)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy_log = accuracy_score(y_test, y_pred_log)\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"LOGISTIC REGRESSION: INCOME REFUSAL PREDICTION\")\n",
        "print(\"=\"*60)\n",
        "print(f\"\\nModel Accuracy: {accuracy_log:.3f} ({accuracy_log*100:.1f}%)\")\n",
        "print(f\"Interpretation: The model correctly predicts refusal {accuracy_log*100:.1f}% of the time.\")\n",
        "\n",
        "# Display model coefficients\n",
        "print(f\"\\nModel Coefficients (interpretation):\")\n",
        "for feature, coef in zip(X.columns, log_reg.coef_[0]):\n",
        "    print(f\"  {feature}: {coef:.4f}\")\n",
        "    if coef > 0:\n",
        "        print(f\"    ↑ Increases probability of refusing income questions\")\n",
        "    elif coef < 0:\n",
        "        print(f\"    ↓ Decreases probability of refusing income questions\")\n",
        "\n",
        "print(f\"\\nIntercept (baseline): {log_reg.intercept_[0]:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.8 Build K-Nearest Neighbors (KNN) Model\n",
        "\n",
        "KNN is a non-linear classifier that works well for comparison with logistic regression."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train KNN model with k=5 neighbors\n",
        "knn = KNeighborsClassifier(n_neighbors=5)\n",
        "knn.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred_knn = knn.predict(X_test)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy_knn = accuracy_score(y_test, y_pred_knn)\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"K-NEAREST NEIGHBORS (KNN, k=5): INCOME REFUSAL PREDICTION\")\n",
        "print(\"=\"*60)\n",
        "print(f\"\\nModel Accuracy: {accuracy_knn:.3f} ({accuracy_knn*100:.1f}%)\")\n",
        "print(f\"\\nComparison with Logistic Regression:\")\n",
        "print(f\"  Logistic Regression Accuracy: {accuracy_log*100:.1f}%\")\n",
        "print(f\"  KNN Accuracy: {accuracy_knn*100:.1f}%\")\n",
        "if accuracy_knn > accuracy_log:\n",
        "    print(f\"  ✓ KNN performs better by {(accuracy_knn - accuracy_log)*100:.1f} percentage points\")\n",
        "else:\n",
        "    print(f\"  ✓ Logistic Regression performs better by {(accuracy_log - accuracy_knn)*100:.1f} percentage points\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.9 Confusion Matrices and Classification Metrics\n",
        "\n",
        "Analyze model performance in detail using confusion matrix and other metrics."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compute confusion matrices\n",
        "cm_log = confusion_matrix(y_test, y_pred_log)\n",
        "cm_knn = confusion_matrix(y_test, y_pred_knn)\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"CONFUSION MATRICES\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "print(f\"\\nLogistic Regression Confusion Matrix:\")\n",
        "print(cm_log)\n",
        "print(f\"\\nKNN Confusion Matrix:\")\n",
        "print(cm_knn)\n",
        "\n",
        "# Extract values for interpretation\n",
        "tn_log, fp_log, fn_log, tp_log = cm_log.ravel()\n",
        "tn_knn, fp_knn, fn_knn, tp_knn = cm_knn.ravel()\n",
        "\n",
        "print(f\"\\n\" + \"=\"*60)\n",
        "print(\"DETAILED CLASSIFICATION METRICS\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "print(f\"\\nLOGISTIC REGRESSION:\")\n",
        "print(classification_report(y_test, y_pred_log, target_names=['Answered', 'Refused']))\n",
        "\n",
        "print(f\"\\nK-NEAREST NEIGHBORS:\")\n",
        "print(classification_report(y_test, y_pred_knn, target_names=['Answered', 'Refused']))\n",
        "\n",
        "print(f\"\\nInterpretation Guide:\")\n",
        "print(f\"  - Precision: Of people predicted to refuse, how many actually did?\")\n",
        "print(f\"  - Recall: Of people who actually refused, how many did we identify?\")\n",
        "print(f\"  - F1-Score: Balance between precision and recall\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.10 Visualize Confusion Matrices and Model Performance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize confusion matrices side by side\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(13, 5))\n",
        "\n",
        "# Logistic Regression Confusion Matrix\n",
        "sns.heatmap(cm_log, annot=True, fmt='d', cmap='Blues', ax=ax1, \n",
        "            xticklabels=['Answered', 'Refused'], yticklabels=['Answered', 'Refused'])\n",
        "ax1.set_title('Logistic Regression Confusion Matrix', fontweight='bold', fontsize=12)\n",
        "ax1.set_ylabel('Actual', fontweight='bold')\n",
        "ax1.set_xlabel('Predicted', fontweight='bold')\n",
        "\n",
        "# KNN Confusion Matrix\n",
        "sns.heatmap(cm_knn, annot=True, fmt='d', cmap='Greens', ax=ax2,\n",
        "            xticklabels=['Answered', 'Refused'], yticklabels=['Answered', 'Refused'])\n",
        "ax2.set_title('K-Nearest Neighbors Confusion Matrix', fontweight='bold', fontsize=12)\n",
        "ax2.set_ylabel('Actual', fontweight='bold')\n",
        "ax2.set_xlabel('Predicted', fontweight='bold')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nConfusion Matrix Interpretation:\")\n",
        "print(\"  - Top-left (True Negatives): Correctly predicted answered questions\")\n",
        "print(\"  - Top-right (False Positives): Incorrectly predicted refusal (false alarm)\")\n",
        "print(\"  - Bottom-left (False Negatives): Incorrectly predicted answered (missed refusers)\")\n",
        "print(\"  - Bottom-right (True Positives): Correctly predicted refusers\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.11 Feature Importance Analysis\n",
        "\n",
        "Determine which factors most strongly predict income question refusal."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Extract coefficients from logistic regression\n",
        "coefficients = pd.DataFrame({\n",
        "    'Feature': X.columns,\n",
        "    'Coefficient': log_reg.coef_[0],\n",
        "    'Abs_Coefficient': np.abs(log_reg.coef_[0])\n",
        "}).sort_values('Abs_Coefficient', ascending=False)\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"FEATURE IMPORTANCE: What Predicts Income Refusal?\")\n",
        "print(\"=\"*60)\n",
        "print(coefficients.to_string(index=False))\n",
        "\n",
        "# Visualize feature importance\n",
        "fig, ax = plt.subplots(figsize=(10, 6))\n",
        "colors = ['red' if x < 0 else 'green' for x in coefficients['Coefficient']]\n",
        "ax.barh(coefficients['Feature'], coefficients['Coefficient'], color=colors, alpha=0.7, edgecolor='black')\n",
        "ax.set_xlabel('Logistic Regression Coefficient', fontweight='bold')\n",
        "ax.set_title('Feature Importance for Predicting Income Question Refusal', fontweight='bold', fontsize=12)\n",
        "ax.axvline(x=0, color='black', linestyle='--', linewidth=1)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nInterpretation:\")\n",
        "print(\"  - Green bars (positive): Increase probability of refusing income questions\")\n",
        "print(\"  - Red bars (negative): Decrease probability of refusing income questions\")\n",
        "print(\"  - Magnitude (length): Strength of effect\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.12 Model Prediction Examples\n",
        "\n",
        "Show what the model predicts for different respondent profiles."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create example respondent profiles\n",
        "example_profiles = pd.DataFrame({\n",
        "    'age': [25, 45, 65, 35, 50],\n",
        "    'home_owner': [0, 1, 1, 0, 1],\n",
        "    'education_Unknown': [0, 0, 0, 1, 0]  # Adjust based on your education categories\n",
        "})\n",
        "\n",
        "# Ensure columns match training data\n",
        "for col in X.columns:\n",
        "    if col not in example_profiles.columns:\n",
        "        example_profiles[col] = 0\n",
        "\n",
        "example_profiles = example_profiles[X.columns]\n",
        "\n",
        "# Scale example profiles\n",
        "example_scaled = scaler.transform(example_profiles)\n",
        "\n",
        "# Get predictions and probabilities\n",
        "example_predictions = log_reg.predict(example_scaled)\n",
        "example_probabilities = log_reg.predict_proba(example_scaled)\n",
        "\n",
        "results = pd.DataFrame({\n",
        "    'Age': [25, 45, 65, 35, 50],\n",
        "    'Home Owner': ['No', 'Yes', 'Yes', 'No', 'Yes'],\n",
        "    'Prediction': ['Answers' if p == 0 else 'Refuses' for p in example_predictions],\n",
        "    'Refusal Probability': [f\"{prob[1]:.1%}\" for prob in example_probabilities],\n",
        "    'Confidence': [f\"{max(prob)*100:.1f}%\" for prob in example_probabilities]\n",
        "})\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"MODEL PREDICTIONS FOR EXAMPLE RESPONDENT PROFILES\")\n",
        "print(\"=\"*60)\n",
        "print(results.to_string(index=False))\n",
        "\n",
        "print(\"\\nKey Insight:\")\n",
        "print(\"Does higher education reduce income question refusal?\")\n",
        "print(\"Does home ownership affect willingness to share income information?\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "# SUMMARY AND CONCLUSIONS"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Key Findings Across All Three Parts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"PROJECT SUMMARY: SILENCE CULTURE IN PUBLIC ATTITUDES\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "print(\"\\n\" + \"─\"*80)\n",
        "print(\"PART 1: GLOBAL SILENCE HIERARCHY\")\n",
        "print(\"─\"*80)\n",
        "print(\"\\nKey Finding:\")\n",
        "if not silence_df.empty:\n",
        "    print(f\"  Most taboo topic: {silence_df.iloc[0]['Topic']}\")\n",
        "    print(f\"  Easiest to discuss: {silence_df.iloc[-1]['Topic']}\")\n",
        "    print(f\"\\nInterpretation:\")\n",
        "    print(f\"  Silence varies significantly by topic (Mean={silence_df['Silence Score'].mean():.1f}%, SD={silence_df['Silence Score'].std():.1f}%)\")\n",
        "    print(f\"  Some topics are universally discussed, others have strong taboos\")\n",
        "\n",
        "print(\"\\n\" + \"─\"*80)\n",
        "print(\"PART 2: GENDER ROLE DISCUSSION PATTERNS BY COUNTRY\")\n",
        "print(\"─\"*80)\n",
        "print(\"\\nKey Finding:\")\n",
        "if not country_refusal_df.empty:\n",
        "    print(f\"  Countries vary in willingness to discuss gender roles\")\n",
        "    print(f\"  Avoidance range: {country_refusal_df['Avoidance Rate (%)'].min():.1f}% to {country_refusal_df['Avoidance Rate (%)'].max():.1f}%\")\n",
        "    print(f\"\\nInterpretation:\")\n",
        "    print(f\"  Cultural differences in how openly gender issues are discussed\")\n",
        "    print(f\"  Some societies show high conformity with high silence (social pressure)\")\n",
        "    print(f\"  Others show disagreement suggesting diverse viewpoints\")\n",
        "\n",
        "print(\"\\n\" + \"─\"*80)\n",
        "print(\"PART 3: PREDICTING INCOME QUESTION REFUSAL\")\n",
        "print(\"─\"*80)\n",
        "print(f\"\\nKey Finding:\")\n",
        "print(f\"  Logistic Regression Accuracy: {accuracy_log*100:.1f}%\")\n",
        "print(f\"  KNN Accuracy: {accuracy_knn*100:.1f}%\")\n",
        "print(f\"\\nInterpretation:\")\n",
        "print(f\"  Education, age, and home ownership predict income disclosure behavior\")\n",
        "if not coefficients.empty:\n",
        "    top_predictor = coefficients.iloc[0]\n",
        "    print(f\"  Strongest predictor: {top_predictor['Feature']}\")\n",
        "print(f\"  Some people systematically avoid income questions regardless of context\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"OVERALL CONCLUSION\")\n",
        "print(\"=\"*80)\n",
        "print(\"\"\"\n",
        "Public willingness to discuss sensitive topics varies significantly:\n",
        "1. By TOPIC - Some issues are universally taboo, others openly discussed\n",
        "2. By COUNTRY - Cultural norms shape discussion comfort levels  \n",
        "3. By INDIVIDUAL - Demographic factors predict information disclosure\n",
        "\n",
        "This 'silence culture' has implications for:\n",
        "- Survey research quality and missing data patterns\n",
        "- Social policy: Which issues are politically difficult to address?\n",
        "- Social cohesion: Do silences reflect consensus or suppressed conflict?\n",
        "\"\"\")\n",
        "\n",
        "print(\"=\"*80)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "# Part 2: Advanced Analysis of the Culture of Silence"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. \"Middle-Class Shame\": The New Economic Silence\n",
        "\n",
        "Goal:\n",
        "- Measure the correlation between **self-defined social class** (Q158) and **missing / refusal in income questions** (Q173–Q174).\n",
        "- Compare this relationship across years (2002 vs 2012 vs 2022).\n",
        "- Hypothesis: Today (2022), low self-classed people (e.g. “lower class”, “working class”) are **more likely** to refuse income questions than in 2002, because poverty is seen more as a personal failure than fate.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from scipy.stats import pearsonr, t\n",
        "from sklearn.metrics import roc_curve, auc\n",
        "\n",
        "sns.set_style(\"whitegrid\")\n",
        "plt.rcParams[\"figure.figsize\"] = (10, 6)\n",
        "\n",
        "# Load data for three years\n",
        "# IMPORTANT: update filenames to your actual paths\n",
        "df_2002 = pd.read_csv(\"2002Data.csv\")\n",
        "df_2012 = pd.read_csv(\"2012Data.csv\")\n",
        "df_2022 = pd.read_csv(\"2022Data.csv\")\n",
        "\n",
        "print(\"2002 shape:\", df_2002.shape)\n",
        "print(\"2012 shape:\", df_2012.shape)\n",
        "print(\"2022 shape:\", df_2022.shape)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4.1 Construct social class and income refusal indicators\n",
        "\n",
        "We need:\n",
        "- `social_class_numeric`: a numeric scale for self-reported social class (Q158).\n",
        "- `income_refusal`: binary indicator:\n",
        "  - 1 = refused / missing in both Q173 and Q174\n",
        "  - 0 = answered at least one income question\n",
        "\n",
        "The exact text categories may differ by year/dataset, so the mapping here is illustrative and should be adjusted to your codes.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def map_social_class_to_numeric(series):\n",
        "    \"\"\"\n",
        "    Map self-reported social class (Q158) to a numeric scale.\n",
        "    Example mapping:\n",
        "        \"lower class\"          -> 1\n",
        "        \"working class\"        -> 2\n",
        "        \"lower middle class\"   -> 3\n",
        "        \"middle class\"         -> 4\n",
        "        \"upper middle class\"   -> 5\n",
        "        \"upper class\"          -> 6\n",
        "\n",
        "    IMPORTANT:\n",
        "    - Replace keys in mapping with the actual labels in your dataset.\n",
        "    \"\"\"\n",
        "    mapping = {\n",
        "        \"lower class\": 1,\n",
        "        \"working class\": 2,\n",
        "        \"lower middle class\": 3,\n",
        "        \"middle class\": 4,\n",
        "        \"upper middle class\": 5,\n",
        "        \"upper class\": 6\n",
        "    }\n",
        "    series_str = series.astype(str).str.lower()\n",
        "    mapped = series_str.map(mapping)\n",
        "    return mapped\n",
        "\n",
        "\n",
        "def build_income_refusal(df, q173_col=\"Q173\", q174_col=\"Q174\"):\n",
        "    \"\"\"\n",
        "    Build a binary variable 'income_refusal':\n",
        "    - 1 if respondent refused or missing in BOTH Q173 and Q174\n",
        "    - 0 otherwise.\n",
        "\n",
        "    'Refusal' is detected via:\n",
        "    - NaN\n",
        "    - textual responses containing 'refuse', 'don\\'t know', 'cannot choose', etc.\n",
        "\n",
        "    IMPORTANT:\n",
        "    - Update column names (q173_col, q174_col) to match your dataset.\n",
        "    - Update refusal_keywords if your labels are coded differently.\n",
        "    \"\"\"\n",
        "    refusal_keywords = [\"refuse\", \"don’t know\", \"don't know\", \"cannot choose\", \"cannot decide\"]\n",
        "\n",
        "    def is_refusal(value):\n",
        "        if pd.isna(value):\n",
        "            return True\n",
        "        text = str(value).lower()\n",
        "        return any(kw in text for kw in refusal_keywords)\n",
        "\n",
        "    # Initialize with zeros\n",
        "    income_refusal = pd.Series(0, index=df.index, dtype=int)\n",
        "\n",
        "    for idx, row in df.iterrows():\n",
        "        val_173 = row[q173_col] if q173_col in df.columns else np.nan\n",
        "        val_174 = row[q174_col] if q174_col in df.columns else np.nan\n",
        "\n",
        "        if is_refusal(val_173) and is_refusal(val_174):\n",
        "            income_refusal.loc[idx] = 1\n",
        "\n",
        "    return income_refusal\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4.2 Compute correlation between class and income refusal (per year)\n",
        "\n",
        "We now:\n",
        "1. Create `social_class_numeric` from Q158.\n",
        "2. Create `income_refusal` from Q173–Q174.\n",
        "3. Compute **Pearson correlation** between these two variables.\n",
        "4. Build **95% confidence intervals** for the correlation using a t-based approach.\n",
        "\n",
        "We will repeat this for 2002, 2012, 2022 and compare.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def prepare_class_and_refusal(df,\n",
        "                              class_col=\"Q158\",\n",
        "                              q173_col=\"Q173\",\n",
        "                              q174_col=\"Q174\",\n",
        "                              year_label=\"unknown\"):\n",
        "    \"\"\"\n",
        "    Prepare a clean DataFrame with:\n",
        "    - social_class_numeric\n",
        "    - income_refusal\n",
        "\n",
        "    Returns a DataFrame with no missing in these two variables.\n",
        "    \"\"\"\n",
        "    # Social class mapping\n",
        "    if class_col not in df.columns:\n",
        "        raise ValueError(f\"{class_col} not found in {year_label} dataset\")\n",
        "\n",
        "    social_class_numeric = map_social_class_to_numeric(df[class_col])\n",
        "\n",
        "    # Income refusal\n",
        "    income_refusal = build_income_refusal(df, q173_col=q173_col, q174_col=q174_col)\n",
        "\n",
        "    tmp = pd.DataFrame({\n",
        "        \"social_class_numeric\": social_class_numeric,\n",
        "        \"income_refusal\": income_refusal\n",
        "    })\n",
        "\n",
        "    # Drop rows with missing social class\n",
        "    tmp_clean = tmp.dropna(subset=[\"social_class_numeric\"])\n",
        "\n",
        "    print(f\"{year_label}: usable rows = {len(tmp_clean)}\")\n",
        "    return tmp_clean\n",
        "\n",
        "\n",
        "def correlation_with_ci(x, y, alpha=0.05):\n",
        "    \"\"\"\n",
        "    Compute Pearson correlation + (1-alpha)% CI using Fisher z-transform.\n",
        "    \"\"\"\n",
        "    r, p_value = pearsonr(x, y)\n",
        "    n = len(x)\n",
        "\n",
        "    if n <= 3:\n",
        "        return r, p_value, (np.nan, np.nan)\n",
        "\n",
        "    # Fisher z\n",
        "    z = np.arctanh(r)\n",
        "    se = 1 / np.sqrt(n - 3)\n",
        "    z_crit = t.ppf(1 - alpha/2, n - 3)  # approximate with t\n",
        "\n",
        "    z_low = z - z_crit * se\n",
        "    z_high = z + z_crit * se\n",
        "\n",
        "    r_low = np.tanh(z_low)\n",
        "    r_high = np.tanh(z_high)\n",
        "\n",
        "    return r, p_value, (r_low, r_high)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# IMPORTANT: Update column names if different in 2002/2012/2022\n",
        "data_2002 = prepare_class_and_refusal(df_2002,\n",
        "                                      class_col=\"Q158\",\n",
        "                                      q173_col=\"Q173\",\n",
        "                                      q174_col=\"Q174\",\n",
        "                                      year_label=\"2002\")\n",
        "\n",
        "data_2012 = prepare_class_and_refusal(df_2012,\n",
        "                                      class_col=\"Q158\",\n",
        "                                      q173_col=\"Q173\",\n",
        "                                      q174_col=\"Q174\",\n",
        "                                      year_label=\"2012\")\n",
        "\n",
        "data_2022 = prepare_class_and_refusal(df_2022,\n",
        "                                      class_col=\"Q158\",\n",
        "                                      q173_col=\"Q173\",\n",
        "                                      q174_col=\"Q174\",\n",
        "                                      year_label=\"2022\")\n",
        "\n",
        "# Compute correlations with CI\n",
        "results_corr = []\n",
        "\n",
        "for year, data in [(\"2002\", data_2002), (\"2012\", data_2012), (\"2022\", data_2022)]:\n",
        "    r, p, (low, high) = correlation_with_ci(data[\"social_class_numeric\"],\n",
        "                                            data[\"income_refusal\"])\n",
        "    results_corr.append({\n",
        "        \"year\": year,\n",
        "        \"r\": r,\n",
        "        \"p_value\": p,\n",
        "        \"ci_low\": low,\n",
        "        \"ci_high\": high\n",
        "    })\n",
        "\n",
        "corr_df = pd.DataFrame(results_corr)\n",
        "print(\"Correlation between social class and income refusal:\")\n",
        "print(corr_df)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4.3 Interpretation and visualization\n",
        "\n",
        "- If **r > 0**: higher self-class → **more** refusal to report income.\n",
        "- If **r < 0**: higher self-class → **less** refusal (i.e., lower classes are more silent).\n",
        "- If |r| increases over time, the relationship between class and silence is strengthening.\n",
        "\n",
        "We also visualize the correlation per year with error bars (CI).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots()\n",
        "\n",
        "ax.errorbar(corr_df[\"year\"],\n",
        "            corr_df[\"r\"],\n",
        "            yerr=[corr_df[\"r\"] - corr_df[\"ci_low\"],\n",
        "                  corr_df[\"ci_high\"] - corr_df[\"r\"]],\n",
        "            fmt=\"o\",\n",
        "            capsize=5,\n",
        "            color=\"darkred\")\n",
        "\n",
        "ax.axhline(0, color=\"black\", linestyle=\"--\", linewidth=1)\n",
        "ax.set_xlabel(\"Year\")\n",
        "ax.set_ylabel(\"Pearson r (class vs income refusal)\")\n",
        "ax.set_title(\"Correlation Between Self-Class and Income Silence Over Time\")\n",
        "\n",
        "plt.show()\n",
        "\n",
        "for _, row in corr_df.iterrows():\n",
        "    print(f\"Year {row['year']}: r={row['r']:.3f}, 95% CI=({row['ci_low']:.3f}, {row['ci_high']:.3f}), p={row['p_value']:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. \"Political Spiral of Silence\": Are We More Suspicious Now?\n",
        "\n",
        "Goal:\n",
        "- Build a **logistic regression classifier** that predicts who refuses to answer the “which party did you vote for” question (Q147).\n",
        "- Features:\n",
        "  - Region (center/periphery) – from place of residence (e.g., `region` or derived from locality code)\n",
        "  - Age\n",
        "  - Religion / religiosity (e.g., Q143–Q145)\n",
        "- Compare model performance (especially **accuracy, ROC, AUC**) between:\n",
        "  - 2012 data\n",
        "  - 2022 data\n",
        "\n",
        "Hypothesis:\n",
        "- In 2022, political climate is more polarized and suspicious.\n",
        "- The **accuracy** of the model in 2022 is higher → it is easier to predict “political silencers” using demographic profiles.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 5.1 Construct target: political refusal (Q147)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import (accuracy_score, confusion_matrix,\n",
        "                             classification_report, roc_curve, auc)\n",
        "\n",
        "def build_political_refusal(df, q147_col=\"Q147\"):\n",
        "    \"\"\"\n",
        "    Build binary variable 'political_refusal':\n",
        "    - 1 if respondent refused / 'don't know' / 'cannot choose' / missing for Q147 (party voted).\n",
        "    - 0 if they provided a valid party answer.\n",
        "\n",
        "    IMPORTANT: Update refusal keywords and party coding as needed.\n",
        "    \"\"\"\n",
        "    refusal_keywords = [\"refuse\", \"don’t know\", \"don't know\", \"cannot choose\", \"cannot decide\", \"blank\"]\n",
        "\n",
        "    def is_refusal(value):\n",
        "        if pd.isna(value):\n",
        "            return True\n",
        "        text = str(value).lower()\n",
        "        # if it contains generic refusal / don't know text -> refusal\n",
        "        if any(kw in text for kw in refusal_keywords):\n",
        "            return True\n",
        "        # Otherwise we treat it as a party choice\n",
        "        return False\n",
        "\n",
        "    target = df[q147_col].apply(is_refusal).astype(int)\n",
        "    return target\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 5.2 Feature construction\n",
        "\n",
        "We create:\n",
        "- `region` (categorical): center / periphery / other.\n",
        "  - This can be derived from locality or pre-coded region column.\n",
        "- `age` (numerical): from year of birth or direct age.\n",
        "- `religion` or `religiosity` (categorical): from Q143 / Q144 / Q145.\n",
        "\n",
        "Because the exact column names and coding differ, we use placeholder names:\n",
        "- `region_col = \"region\"`\n",
        "- `age_col = \"age\"`\n",
        "- `religion_col = \"religion\"`\n",
        "\n",
        "You can replace them with the correct column names in your data.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def prepare_political_dataset(df,\n",
        "                              year_label=\"unknown\",\n",
        "                              q147_col=\"Q147\",\n",
        "                              region_col=\"region\",\n",
        "                              age_col=\"age\",\n",
        "                              religion_col=\"religion\"):\n",
        "    \"\"\"\n",
        "    Prepare X, y for political refusal classification.\n",
        "\n",
        "    - y: political_refusal\n",
        "    - X: region (categorical), age (numeric), religion (categorical)\n",
        "\n",
        "    IMPORTANT:\n",
        "    - Make sure region_col, age_col, religion_col exist or derive them before calling this function.\n",
        "    \"\"\"\n",
        "    # Target\n",
        "    if q147_col not in df.columns:\n",
        "        raise ValueError(f\"{q147_col} not found in {year_label} dataset\")\n",
        "\n",
        "    y = build_political_refusal(df, q147_col=q147_col)\n",
        "\n",
        "    # Features (create minimal working example, you can enrich later)\n",
        "    X = pd.DataFrame(index=df.index)\n",
        "\n",
        "    # Age\n",
        "    if age_col in df.columns:\n",
        "        X[\"age\"] = pd.to_numeric(df[age_col], errors=\"coerce\")\n",
        "    else:\n",
        "        # If you only have year of birth, you can compute age:\n",
        "        # X[\"age\"] = 2022 - pd.to_numeric(df[\"birth_year\"], errors=\"coerce\")\n",
        "        X[\"age\"] = np.nan\n",
        "\n",
        "    # Region\n",
        "    if region_col in df.columns:\n",
        "        X[\"region\"] = df[region_col].astype(str)\n",
        "    else:\n",
        "        # Dummy fallback: center vs periphery based on locality if available\n",
        "        # For now use placeholder\n",
        "        X[\"region\"] = \"unknown\"\n",
        "\n",
        "    # Religion / religiosity\n",
        "    if religion_col in df.columns:\n",
        "        X[\"religion\"] = df[religion_col].astype(str)\n",
        "    else:\n",
        "        X[\"religion\"] = \"unknown\"\n",
        "\n",
        "    # Drop rows with missing age (for simplicity)\n",
        "    mask_valid = X[\"age\"].notna()\n",
        "    X = X[mask_valid]\n",
        "    y = y[mask_valid]\n",
        "\n",
        "    print(f\"{year_label}: usable rows for political model = {len(X)}\")\n",
        "    return X, y\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 5.3 Train and evaluate logistic regression (2012 vs 2022)\n",
        "\n",
        "We will:\n",
        "- Split into train/test (e.g. 80/20, stratified).\n",
        "- Use a **pipeline**:\n",
        "  - ColumnTransformer:\n",
        "    - scale numeric features (`age`)\n",
        "    - one-hot encode categorical features (`region`, `religion`)\n",
        "  - LogisticRegression classifier\n",
        "- Compute:\n",
        "  - Accuracy\n",
        "  - Confusion matrix\n",
        "  - Classification report\n",
        "  - ROC curve and AUC\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_logistic_for_year(df, year_label,\n",
        "                            q147_col=\"Q147\",\n",
        "                            region_col=\"region\",\n",
        "                            age_col=\"age\",\n",
        "                            religion_col=\"religion\"):\n",
        "    X, y = prepare_political_dataset(df,\n",
        "                                     year_label=year_label,\n",
        "                                     q147_col=q147_col,\n",
        "                                     region_col=region_col,\n",
        "                                     age_col=age_col,\n",
        "                                     religion_col=religion_col)\n",
        "\n",
        "    # Define feature types\n",
        "    numeric_features = [\"age\"]\n",
        "    categorical_features = [\"region\", \"religion\"]\n",
        "\n",
        "    preprocessor = ColumnTransformer(\n",
        "        transformers=[\n",
        "            (\"num\", StandardScaler(), numeric_features),\n",
        "            (\"cat\", OneHotEncoder(handle_unknown=\"ignore\"), categorical_features)\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    clf = Pipeline(steps=[\n",
        "        (\"preprocessor\", preprocessor),\n",
        "        (\"model\", LogisticRegression(max_iter=1000, random_state=42))\n",
        "    ])\n",
        "\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        X, y, test_size=0.2, stratify=y, random_state=42\n",
        "    )\n",
        "\n",
        "    clf.fit(X_train, y_train)\n",
        "    y_pred = clf.predict(X_test)\n",
        "    y_proba = clf.predict_proba(X_test)[:, 1]\n",
        "\n",
        "    acc = accuracy_score(y_test, y_pred)\n",
        "    cm = confusion_matrix(y_test, y_pred)\n",
        "    report = classification_report(y_test, y_pred, target_names=[\"Answered\", \"Refused\"])\n",
        "\n",
        "    fpr, tpr, thresholds = roc_curve(y_test, y_proba)\n",
        "    roc_auc = auc(fpr, tpr)\n",
        "\n",
        "    results = {\n",
        "        \"year\": year_label,\n",
        "        \"accuracy\": acc,\n",
        "        \"confusion_matrix\": cm,\n",
        "        \"classification_report\": report,\n",
        "        \"fpr\": fpr,\n",
        "        \"tpr\": tpr,\n",
        "        \"roc_auc\": roc_auc\n",
        "    }\n",
        "    return results\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train for 2012 and 2022\n",
        "results_2012 = train_logistic_for_year(df_2012, \"2012\",\n",
        "                                       q147_col=\"Q147\",\n",
        "                                       region_col=\"region\",\n",
        "                                       age_col=\"age\",\n",
        "                                       religion_col=\"religion\")\n",
        "\n",
        "results_2022 = train_logistic_for_year(df_2022, \"2022\",\n",
        "                                       q147_col=\"Q147\",\n",
        "                                       region_col=\"region\",\n",
        "                                       age_col=\"age\",\n",
        "                                       religion_col=\"religion\")\n",
        "\n",
        "print(\"2012 accuracy:\", results_2012[\"accuracy\"])\n",
        "print(\"2022 accuracy:\", results_2022[\"accuracy\"])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 5.4 Compare metrics & plot ROC curves"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Print detailed reports\n",
        "for res in [results_2012, results_2022]:\n",
        "    print(\"=\" * 60)\n",
        "    print(f\"Year {res['year']} – Logistic Regression Performance\")\n",
        "    print(\"=\" * 60)\n",
        "    print(\"Accuracy:\", f\"{res['accuracy']:.3f}\")\n",
        "    print(\"\\nConfusion matrix:\\n\", res[\"confusion_matrix\"])\n",
        "    print(\"\\nClassification report:\\n\", res[\"classification_report\"])\n",
        "\n",
        "# Plot ROC curves\n",
        "plt.figure()\n",
        "plt.plot(results_2012[\"fpr\"], results_2012[\"tpr\"],\n",
        "         label=f\"2012 (AUC = {results_2012['roc_auc']:.3f})\")\n",
        "plt.plot(results_2022[\"fpr\"], results_2022[\"tpr\"],\n",
        "         label=f\"2022 (AUC = {results_2022['roc_auc']:.3f})\")\n",
        "\n",
        "plt.plot([0, 1], [0, 1], \"k--\", label=\"Random baseline\")\n",
        "\n",
        "plt.xlabel(\"False Positive Rate\")\n",
        "plt.ylabel(\"True Positive Rate\")\n",
        "plt.title(\"ROC Curve – Political Refusal Classifier (2012 vs 2022)\")\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nInterpretation:\")\n",
        "print(\"- Higher AUC indicates better discrimination between 'answer' and 'refuse'.\")\n",
        "print(\"- If 2022 AUC >> 2012 AUC, it suggests that demographic profile\")\n",
        "print(\"  is more strongly linked to political silence in 2022.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Polarization: Has the Distribution Changed?\n",
        "\n",
        "Goal:\n",
        "- Check whether Israelis’ opinions have become more **polarized** (fewer in the middle, more at the extremes).\n",
        "- Use:\n",
        "  - Histograms and kernel density plots\n",
        "  - Standard deviation (σ) as a simple spread measure\n",
        "- Questions:\n",
        "  - Q74–Q77 (family values)\n",
        "  - Q121 (women in politics)\n",
        "\n",
        "Approach:\n",
        "1. Recode responses into a numerical Likert scale:\n",
        "   - Strongly disagree = -2\n",
        "   - Disagree = -1\n",
        "   - Neither = 0\n",
        "   - Agree = 1\n",
        "   - Strongly agree = 2\n",
        "2. Plot distributions for 2002 vs 2022.\n",
        "3. Compare:\n",
        "   - shape (bell-shaped vs bi-modal)\n",
        "   - standard deviation (higher σ → more spread / polarization).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 6.1 Helper: Likert mapping\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def likert_to_numeric(series):\n",
        "    \"\"\"\n",
        "    Map Likert-scale responses to numeric values.\n",
        "\n",
        "    Example:\n",
        "        'strongly disagree' -> -2\n",
        "        'disagree'          -> -1\n",
        "        'neither ...'       -> 0\n",
        "        'agree'             -> 1\n",
        "        'strongly agree'    -> 2\n",
        "\n",
        "    IMPORTANT:\n",
        "    - Update keywords according to actual language/labels in the data.\n",
        "    - For Hebrew labels, you may map by code values instead.\n",
        "    \"\"\"\n",
        "    mapping = {\n",
        "        \"strongly disagree\": -2,\n",
        "        \"disagree\": -1,\n",
        "        \"neither agree nor disagree\": 0,\n",
        "        \"agree\": 1,\n",
        "        \"strongly agree\": 2\n",
        "    }\n",
        "\n",
        "    def map_single(v):\n",
        "        if pd.isna(v):\n",
        "            return np.nan\n",
        "        text = str(v).lower()\n",
        "        for key, val in mapping.items():\n",
        "            if key in text:\n",
        "                return val\n",
        "        # If not matched, treat as missing\n",
        "        return np.nan\n",
        "\n",
        "    return series.apply(map_single)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 6.2 Extract numeric responses for Q74–Q77 and Q121\n",
        "\n",
        "We will:\n",
        "- Convert each question to numeric (using `likert_to_numeric`).\n",
        "- Compute:\n",
        "  - mean\n",
        "  - standard deviation\n",
        "- Plot histograms + density curves for 2002 vs 2022.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "family_questions = [\"Q74\", \"Q75\", \"Q76\", \"Q77\"]\n",
        "politics_question = \"Q121\"\n",
        "\n",
        "def prepare_likert_for_year(df, questions, year_label):\n",
        "    out = {}\n",
        "    for q in questions:\n",
        "        if q in df.columns:\n",
        "            numeric = likert_to_numeric(df[q])\n",
        "            out[q] = numeric.dropna()\n",
        "            print(f\"{year_label}: {q} – n={len(out[q])}, mean={out[q].mean():.2f}, std={out[q].std():.2f}\")\n",
        "        else:\n",
        "            print(f\"{year_label}: {q} not found in dataset\")\n",
        "    return out\n",
        "\n",
        "print(\"=== 2002 Family & Politics ===\")\n",
        "likert_2002_family = prepare_likert_for_year(df_2002, family_questions, \"2002\")\n",
        "likert_2002_politics = prepare_likert_for_year(df_2002, [politics_question], \"2002\")\n",
        "\n",
        "print(\"\\n=== 2022 Family & Politics ===\")\n",
        "likert_2022_family = prepare_likert_for_year(df_2022, family_questions, \"2022\")\n",
        "likert_2022_politics = prepare_likert_for_year(df_2022, [politics_question], \"2022\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 6.3 Plot distributions: example for Q74 and Q121"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def plot_distribution_comparison(series_2002, series_2022, question_label, title_suffix=\"\"):\n",
        "    \"\"\"\n",
        "    Plot histogram + KDE for 2002 vs 2022 for a given question.\n",
        "    \"\"\"\n",
        "    plt.figure(figsize=(10, 5))\n",
        "\n",
        "    sns.histplot(series_2002, bins=5, kde=True, stat=\"density\",\n",
        "                 color=\"steelblue\", alpha=0.5, label=\"2002\")\n",
        "    sns.histplot(series_2022, bins=5, kde=True, stat=\"density\",\n",
        "                 color=\"darkred\", alpha=0.5, label=\"2022\")\n",
        "\n",
        "    plt.xlabel(\"Opinion (Likert numeric: -2 .. 2)\")\n",
        "    plt.ylabel(\"Density\")\n",
        "    plt.title(f\"Distribution of Responses – {question_label} {title_suffix}\")\n",
        "    plt.legend()\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    plt.show()\n",
        "\n",
        "    print(f\"{question_label} – 2002: mean={series_2002.mean():.2f}, std={series_2002.std():.2f}\")\n",
        "    print(f\"{question_label} – 2022: mean={series_2022.mean():.2f}, std={series_2022.std():.2f}\")\n",
        "    print(\"Higher std suggests more spread / potential polarization.\\n\")\n",
        "\n",
        "\n",
        "# Example: Q74 (family values) if present in both years\n",
        "if \"Q74\" in likert_2002_family and \"Q74\" in likert_2022_family:\n",
        "    plot_distribution_comparison(likert_2002_family[\"Q74\"],\n",
        "                                 likert_2022_family[\"Q74\"],\n",
        "                                 question_label=\"Q74 – Family Value Statement\")\n",
        "\n",
        "# Example: Q121 (women in politics) if present\n",
        "q121_2002 = likert_2002_politics.get(\"Q121\")\n",
        "q121_2022 = likert_2022_politics.get(\"Q121\")\n",
        "\n",
        "if q121_2002 is not None and q121_2022 is not None:\n",
        "    plot_distribution_comparison(q121_2002, q121_2022,\n",
        "                                 question_label=\"Q121 – Women in Politics\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 6.4 Optional: Aggregate polarization index across questions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def polarization_summary(likert_dict, label):\n",
        "    \"\"\"\n",
        "    Compute an aggregated polarization summary (average std across questions).\n",
        "    \"\"\"\n",
        "    stds = []\n",
        "    for q, s in likert_dict.items():\n",
        "        if len(s) > 0:\n",
        "            stds.append(s.std())\n",
        "    if len(stds) == 0:\n",
        "        print(f\"{label}: no data\")\n",
        "        return\n",
        "    print(f\"{label}: average std across questions = {np.mean(stds):.3f}\")\n",
        "\n",
        "print(\"Family values – polarization summary\")\n",
        "polarization_summary(likert_2002_family, \"2002 family\")\n",
        "polarization_summary(likert_2022_family, \"2022 family\")\n",
        "\n",
        "print(\"\\nPolitics (Q121) – single-question std comparison\")\n",
        "if q121_2002 is not None and q121_2022 is not None:\n",
        "    print(f\"2002: std={q121_2002.std():.3f}\")\n",
        "    print(f\"2022: std={q121_2022.std():.3f}\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
